{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-1.6.0-cp38-none-macosx_10_9_x86_64.whl (97.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 97.5 MB 20.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting torchvision\n",
      "  Downloading torchvision-0.7.0-cp38-cp38-macosx_10_9_x86_64.whl (387 kB)\n",
      "\u001b[K     |████████████████████████████████| 387 kB 13.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/hwangjeahong/opt/miniconda3/lib/python3.8/site-packages (from torch) (1.19.1)\n",
      "Collecting future\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[K     |████████████████████████████████| 829 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /Users/hwangjeahong/opt/miniconda3/lib/python3.8/site-packages (from torchvision) (7.2.0)\n",
      "Building wheels for collected packages: future\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491058 sha256=5ab05427fe8093e4e24704c9c14cc70c6a40073bfe94bbd934a3961f57847931\n",
      "  Stored in directory: /Users/hwangjeahong/Library/Caches/pip/wheels/8e/70/28/3d6ccd6e315f65f245da085482a2e1c7d14b90b30f239e2cf4\n",
      "Successfully built future\n",
      "Installing collected packages: future, torch, torchvision\n",
      "Successfully installed future-0.18.2 torch-1.6.0 torchvision-0.7.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2117, 0.2679, 0.6813],\n",
      "        [0.4061, 0.2753, 0.0444],\n",
      "        [0.4306, 0.8344, 0.4668],\n",
      "        [0.6962, 0.9548, 0.6357],\n",
      "        [0.5529, 0.5767, 0.1538]])\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb2885f94b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
    "y_data = [[0],[0],[0],[1],[1],[1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2,1), requires_grad = True)\n",
    "b = torch.zeros(1, requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W)+b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_set = pd.read_csv(\"fraction3.csv\",encoding=\"CP949\")\n",
    "input_set = pd.read_csv(\"data_regular.csv\", encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = []\n",
    "data = []\n",
    "for i in range(0,424):\n",
    "    data.clear()\n",
    "    a = input_set.loc[i].values\n",
    "    b = a[4:]\n",
    "    for item in b:\n",
    "        data.append(item)\n",
    "    datas.append(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([424, 15])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_o = output_set[\"20~29세\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_o = twenty_o.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty = []\n",
    "for item in twenty_o:\n",
    "    twenty.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y20_train = torch.FloatTensor(twenty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+e^-Z)\n",
    "def relu(x):\n",
    "    return(0,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dim):\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dim)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        parameters['W'+str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])\n",
    "        parameters['b'+str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    z = np.dot(W,A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    assert (A.shpae == (W.shape[0], A_prev.shpae[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_forward(X, parameters):\n",
    "    cache = []\n",
    "    A = X\n",
    "    L = len(parameters)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b'+str(l)], activation = 'relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "        AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b'+str(L)], activation = 'sigmoid')\n",
    "        caches.append(cache)\n",
    "        \n",
    "        assert(AL.shape == (1,X.shape[1]))\n",
    "        \n",
    "        return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shpae[1]\n",
    "    \n",
    "    cost = (-1/m) * np.sum((np.multiply(Y,np.log(AL)) + np.multiply(1-Y, np.log(1-AL))))\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1/m*np.dot(dZ, A_prev.T)\n",
    "    db = 1/m*np.sum(dZ, axis=1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    assert(dW.shape == W.shape)\n",
    "    assert(db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "    if activation == 'relu':\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_layer_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    dAL = -(np.divide(Y,AL) - np.divide(1-Y, 1-AL))\n",
    "    \n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation=\"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters)\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
